"""
ContextXä¸Šä¸‹æ–‡å¤„ç†å™¨

è´Ÿè´£å°†å›¢é˜Ÿè®°å¿†ã€é…ç½®å’Œseven_stage_frameworkæ¨¡æ¿è½¬æ¢ä¸ºç»“æ„åŒ–çš„ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒï¼š
- è®°å¿†å†…å®¹çš„æ™ºèƒ½èšåˆå’Œè¿‡æ»¤
- seven_stage_frameworkä¸ƒé˜¶æ®µæ¨¡å—çš„ç»„è£…
- æ··åˆæ¨¡å¼ï¼šè®°å¿†+ä¸ƒé˜¶æ®µæ¡†æ¶çš„èåˆ
- åŸºäºåœºæ™¯çš„ä¸Šä¸‹æ–‡å®šåˆ¶
- é›†æˆå¢å¼ºè¯„åˆ†ç®—æ³•ç”¨äºæ™ºèƒ½è®°å¿†é€‰æ‹©
"""

import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum

from .markdown_engine import MarkdownEngine, MemoryEntry, ContextSection
from .directory_manager import DirectoryManager

# å¢å¼ºè¯„åˆ†ç®—æ³•é…ç½®
ENABLE_ENHANCED_SCORING = True  # æ˜¯å¦å¯ç”¨å¢å¼ºè¯„åˆ†ç®—æ³•
ENHANCED_SCORING_DEBUG = True  # æ˜¯å¦æ˜¾ç¤ºå¢å¼ºè¯„åˆ†çš„è°ƒè¯•ä¿¡æ¯


class ContextMode(Enum):
    """ä¸Šä¸‹æ–‡ç”Ÿæˆæ¨¡å¼"""
    MEMORY_ONLY = "memory_only"           # ä»…ä½¿ç”¨è®°å¿†
    FRAMEWORK_ONLY = "framework_only"     # ä»…ä½¿ç”¨ä¸ƒé˜¶æ®µæ¡†æ¶
    HYBRID = "hybrid"                     # æ··åˆæ¨¡å¼ï¼šè®°å¿†+æ¡†æ¶


class MemoryType(Enum):
    """è®°å¿†ç±»å‹"""
    DECLARATIVE = "declarative"           # å£°æ˜æ€§è®°å¿†
    PROCEDURAL = "procedural"             # ç¨‹åºæ€§è®°å¿†
    EPISODIC = "episodic"                 # æƒ…æ™¯æ€§è®°å¿†
    ALL = "all"                           # æ‰€æœ‰è®°å¿†


@dataclass
class ContextGenerationConfig:
    """ä¸Šä¸‹æ–‡ç”Ÿæˆé…ç½®"""
    team_name: str
    project_name: Optional[str] = None  # æ–°å¢ï¼šé¡¹ç›®åç§°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å›¢é˜Ÿçº§åˆ«è®°å¿†
    mode: ContextMode = ContextMode.HYBRID
    
    # è®°å¿†ç›¸å…³é…ç½®
    include_memory_types: List[MemoryType] = field(default_factory=lambda: [MemoryType.ALL])
    max_memory_items: int = 50
    memory_importance_threshold: int = 2
    include_team_memories: bool = True  # æ–°å¢ï¼šæ˜¯å¦åŒ…å«å›¢é˜Ÿçº§åˆ«çš„é€šç”¨è®°å¿†
    
    # æ¡†æ¶ç›¸å…³é…ç½®
    include_framework_stages: List[str] = field(default_factory=lambda: [
        "requirements", "business-model", "solution", "structure", 
        "tasks", "common-tasks", "constraints"
    ])
    
    # è¿‡æ»¤æ¡ä»¶ (project_scopeä¿ç•™ç”¨äºå‘åå…¼å®¹)
    project_scope: Optional[str] = None
    time_range: Optional[Tuple[str, str]] = None
    memory_filters: Dict[str, Any] = field(default_factory=dict)


@dataclass
class GeneratedContext:
    """ç”Ÿæˆçš„ä¸Šä¸‹æ–‡ç»“æœ"""
    team_name: str
    mode: ContextMode
    content: str
    source_memories: List[str] = field(default_factory=list)
    framework_stages: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    generation_time: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_markdown(self) -> str:
        """è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        # ç›´æ¥è¿”å›å†…å®¹ï¼Œä¸åŒ…å«å…ƒä¿¡æ¯
        return self.content
    
    def to_markdown_with_metadata(self) -> str:
        """è½¬æ¢ä¸ºåŒ…å«å…ƒä¿¡æ¯çš„å®Œæ•´Markdownæ ¼å¼"""
        md_content = [
            f"# {self.team_name.title()} Team Context",
            f"",
            f"**Generation Mode:** {self.mode.value}",
            f"**Generated:** {self.generation_time}",
            ""
        ]
        
        if self.source_memories:
            md_content.extend([
                f"**Memory Sources:** {len(self.source_memories)} items",
                ""
            ])
        
        if self.framework_stages:
            md_content.extend([
                f"**Framework Stages:** {', '.join(self.framework_stages)}",
                ""
            ])
        
        md_content.extend([
            "---",
            "",
            self.content
        ])
        
        if self.metadata:
            md_content.extend([
                "",
                "---",
                "",
                "## Generation Metadata",
                "",
                f"```json",
                json.dumps(self.metadata, indent=2, ensure_ascii=False),
                "```"
            ])
        
        return "\n".join(md_content)
    
    def save_to_file(self, output_path: Union[str, Path]) -> Path:
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        output_path.write_text(self.to_markdown(), encoding='utf-8')
        return output_path


class ContextProcessor:
    """ä¸Šä¸‹æ–‡å¤„ç†å™¨"""
    
    def __init__(self, base_path: Path, enable_optimized_scoring: bool = True):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡å¤„ç†å™¨
        
        Args:
            base_path: å›¢é˜Ÿæ•°æ®æ ¹ç›®å½•
            enable_optimized_scoring: æ˜¯å¦å¯ç”¨ä¼˜åŒ–çš„è¯„åˆ†å¼•æ“
        """
        self.base_path = Path(base_path)
        self.directory_manager = DirectoryManager(base_path)
        self.markdown_engine = MarkdownEngine()
        
        # Seven stage framework è·¯å¾„
        self.framework_path = Path(__file__).parent.parent / "seven_stage_framework"
        
        # åˆå§‹åŒ–ä¼˜åŒ–è¯„åˆ†å¼•æ“
        self.enable_optimized_scoring = enable_optimized_scoring
        self._optimized_scoring_engine = None
        if enable_optimized_scoring:
            try:
                from .optimized_scoring_engine import OptimizedScoringEngine
                cache_dir = self.base_path / ".scoring_cache"
                self._optimized_scoring_engine = OptimizedScoringEngine(cache_dir=cache_dir)
                print("âœ… ä¼˜åŒ–è¯„åˆ†å¼•æ“å·²å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ ä¼˜åŒ–è¯„åˆ†å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹è¯„åˆ†: {e}")
                self.enable_optimized_scoring = False
        
        # é˜¶æ®µæ–‡ä»¶æ˜ å°„
        self.stage_files = {
            "overview": "00_overview.md",
            "requirements": "01_requirements.md",
            "business-model": "02_business_model.md",
            "solution": "03_solution.md",
            "structure": "04_structure.md",
            "tasks": "05_tasks.md",
            "common-tasks": "06_common_tasks.md",
            "constraints": "07_constraints.md"
        }
    
    def generate_context(self, config: ContextGenerationConfig, user_message: str = None) -> GeneratedContext:
        """
        ç”Ÿæˆç»“æ„åŒ–ä¸Šä¸‹æ–‡
        
        Args:
            config: ä¸Šä¸‹æ–‡ç”Ÿæˆé…ç½®
            user_message: ç”¨æˆ·æ¶ˆæ¯ï¼Œç”¨äºæ™ºèƒ½é€‰æ‹©ç›¸å…³è®°å¿†
        
        Returns:
            ç”Ÿæˆçš„ä¸Šä¸‹æ–‡
        """
        # éªŒè¯å›¢é˜Ÿå­˜åœ¨
        if not self.directory_manager.team_exists(config.team_name):
            raise ValueError(f"Team '{config.team_name}' does not exist")
        
        team_path = self.directory_manager.get_team_path(config.team_name)
        
        # æ ¹æ®æ¨¡å¼ç”Ÿæˆä¸Šä¸‹æ–‡
        if config.mode == ContextMode.MEMORY_ONLY:
            return self._generate_memory_only_context(config, team_path, user_message)
        elif config.mode == ContextMode.FRAMEWORK_ONLY:
            return self._generate_framework_only_context(config, team_path)
        elif config.mode == ContextMode.HYBRID:
            return self._generate_hybrid_context(config, team_path, user_message)
        else:
            raise ValueError(f"Unsupported context mode: {config.mode}")
    
    def _generate_memory_only_context(self, config: ContextGenerationConfig, team_path: Path, user_message: str = None) -> GeneratedContext:
        """ç”Ÿæˆä»…åŸºäºè®°å¿†çš„ä¸Šä¸‹æ–‡"""
        memories = self._load_team_memories(team_path, config)
        
        content_sections = []
        
        if memories:
            # å¦‚æœæä¾›äº†ç”¨æˆ·æ¶ˆæ¯ï¼Œæ™ºèƒ½é€‰æ‹©ç›¸å…³è®°å¿†
            if user_message:
                relevant_memories = self._find_relevant_memories_by_message(memories, user_message)
                selected_memories = relevant_memories[:10]  # é™åˆ¶æœ€å¤š10ä¸ªè®°å¿†
            else:
                # å¦åˆ™æŒ‰ç±»å‹ç»„ç»‡è®°å¿†
                selected_memories = memories
            
            if selected_memories:
                for memory in selected_memories:
                    content_sections.extend([
                        f"### {memory.id}",
                        f"**Project:** {memory.project}",
                        f"**Importance:** {'â­' * memory.importance}",
                        f"**Tags:** {', '.join(memory.tags)}",
                        f"**Timestamp:** {memory.timestamp}",
                        "",
                        memory.content,
                        "",
                        "---",
                        ""
                    ])
        
        # å¦‚æœæ²¡æœ‰è®°å¿†ï¼Œç”Ÿæˆç®€æ´çš„å†…å®¹ï¼ˆä¸æ˜¾ç¤º"No memories found"ï¼‰
        if not memories:
            content_sections = [""]  # ç©ºå†…å®¹
        
        return GeneratedContext(
            team_name=config.team_name,
            mode=config.mode,
            content="\n".join(content_sections),
            source_memories=[m.id for m in memories],
            metadata={
                'memory_count': len(memories),
                'memory_types_included': [mt.value for mt in config.include_memory_types]
            }
        )
    
    def _generate_framework_only_context(self, config: ContextGenerationConfig, team_path: Path) -> GeneratedContext:
        """ç”Ÿæˆä»…åŸºäºä¸ƒé˜¶æ®µæ¡†æ¶çš„ä¸Šä¸‹æ–‡"""
        content_sections = []
        
        # åŠ è½½æ¦‚è¿°
        overview_content = self._load_framework_stage("overview")
        if overview_content:
            content_sections.extend([
                overview_content,
                "",
                "---",
                ""
            ])
        
        # åŠ è½½å„é˜¶æ®µå†…å®¹
        included_stages = []
        for stage in config.include_framework_stages:
            stage_content = self._load_framework_stage(stage)
            if stage_content:
                included_stages.append(stage)
                content_sections.extend([
                    stage_content,
                    "",
                    "---",
                    ""
                ])
        
        # å°è¯•åŠ è½½å›¢é˜Ÿè‡ªå®šä¹‰çš„ä¸Šä¸‹æ–‡æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        team_context_sections = self._load_team_context_files(team_path, config.include_framework_stages)
        if team_context_sections:
            content_sections.extend([
                "## Team-Specific Context",
                "",
                *team_context_sections
            ])
        
        return GeneratedContext(
            team_name=config.team_name,
            mode=config.mode,
            content="\n".join(content_sections),
            framework_stages=included_stages,
            metadata={
                'framework_stages_count': len(included_stages),
                'requested_stages': config.include_framework_stages
            }
        )
    
    def _generate_hybrid_context(self, config: ContextGenerationConfig, team_path: Path, user_message: str = None) -> GeneratedContext:
        """ç”Ÿæˆæ··åˆæ¨¡å¼ä¸Šä¸‹æ–‡ï¼ˆè®°å¿†+æ¡†æ¶ï¼‰"""
        memories = self._load_team_memories(team_path, config)
        
        content_sections = []
        used_memory_ids = set()  # è·Ÿè¸ªå·²ä½¿ç”¨çš„è®°å¿†ï¼Œé¿å…é‡å¤
        
        # 0. æ·»åŠ æ··åˆæ¨¡å¼æŒ‡å¯¼æç¤º
        content_sections.extend([
            "å…ˆç†è§£è®°å¿†çš„å†…å®¹ï¼Œå†åŸºäºä¸ƒæ­¥æ¡†æ¶ç»“åˆuser_messageç”Ÿæˆå…·ä½“çš„ç»“æ„åŒ–æç¤ºè¯",
            "",
            "---",
            ""
        ])
        
        # 1. è®°å¿†å†…å®¹ä¼˜å…ˆæ”¾ç½®åœ¨å‰é¢ - ä»…åœ¨æœ‰ç›¸å…³æ€§æ—¶åŠ è½½
        if memories and user_message:
            # åªæœ‰åœ¨æä¾›ç”¨æˆ·æ¶ˆæ¯æ—¶æ‰å°è¯•åŒ¹é…è®°å¿†ï¼Œé¿å…æ— å…³è®°å¿†æ±¡æŸ“ä¸Šä¸‹æ–‡
            relevant_memories = self._find_relevant_memories_by_message(memories, user_message)
            if relevant_memories:
                # é™åˆ¶æœ€å¤š5ä¸ªæœ€ç›¸å…³çš„è®°å¿†
                top_memories = relevant_memories[:5]
                for memory in top_memories:
                    content_sections.extend([
                        f"#å›¢é˜Ÿè®°å¿†",
                        f"### {memory.id}",
                        f"**Project:** {memory.project}",
                        f"**Importance:** {'â­' * memory.importance}",
                        f"**Tags:** {', '.join(memory.tags)}",
                        "",
                        memory.content,
                        "",
                        "---",
                        ""
                    ])
                    used_memory_ids.add(memory.id)
        
        # 2. ä¸ƒé˜¶æ®µæ¡†æ¶ä½œä¸ºä¸»ä½“ç»“æ„
        content_sections.extend([
            "# ä¸ƒæ­¥æ¡†æ¶æ¨¡æ¿å†…å®¹",
            "",
            ""
        ])
        
        included_stages = []
        for stage in config.include_framework_stages:
            stage_content = self._load_framework_stage(stage)
            if stage_content:
                included_stages.append(stage)
                content_sections.extend([
                    stage_content,
                    ""
                ])
                
                # 3. åŠ è½½é¡¹ç›®æˆ–å›¢é˜Ÿè‡ªå®šä¹‰ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœå­˜åœ¨ä¸”æœ‰å®é™…å†…å®¹ï¼‰
                context_content = self._load_context_file(team_path, stage, config)
                if context_content and context_content.strip():
                    content_sections.extend([
                        context_content,
                        ""
                    ])
                
                content_sections.extend(["---", ""])
        
        return GeneratedContext(
            team_name=config.team_name,
            mode=config.mode,
            content="\n".join(content_sections),
            source_memories=list(used_memory_ids),  # åªåŒ…å«å®é™…ä½¿ç”¨çš„è®°å¿†ID
            framework_stages=included_stages,
            metadata={
                'memory_count': len(used_memory_ids),  # å®é™…ä½¿ç”¨çš„è®°å¿†æ•°é‡
                'total_memories_available': len(memories),  # æ€»çš„å¯ç”¨è®°å¿†æ•°é‡
                'framework_stages_count': len(included_stages),
                'hybrid_mode': True
            }
        )
    
    def _load_team_memories(self, team_path: Path, config: ContextGenerationConfig) -> List[MemoryEntry]:
        """åŠ è½½å›¢é˜Ÿæˆ–é¡¹ç›®è®°å¿†"""
        memories = []
        
        # æ ¹æ®é…ç½®å†³å®šåŠ è½½å“ªäº›ç±»å‹çš„è®°å¿†
        memory_types_to_load = config.include_memory_types
        if MemoryType.ALL in memory_types_to_load:
            memory_types_to_load = [MemoryType.DECLARATIVE, MemoryType.PROCEDURAL, MemoryType.EPISODIC]
        
        # å¦‚æœæŒ‡å®šäº†é¡¹ç›®ï¼Œä¼˜å…ˆåŠ è½½é¡¹ç›®çº§åˆ«çš„è®°å¿†
        if config.project_name:
            project_path = team_path / "projects" / config.project_name
            if project_path.exists():
                project_memories = self._load_memories_from_path(project_path, memory_types_to_load, f"project:{config.project_name}")
                memories.extend(project_memories)
        
        # å¦‚æœé…ç½®äº†åŒ…å«å›¢é˜Ÿè®°å¿†ï¼Œæˆ–è€…æ²¡æœ‰æŒ‡å®šé¡¹ç›®ï¼ŒåŠ è½½å›¢é˜Ÿçº§åˆ«çš„è®°å¿†
        if config.include_team_memories or not config.project_name:
            team_memories = self._load_memories_from_path(team_path, memory_types_to_load, "team")
            memories.extend(team_memories)
        
        # å»é‡ï¼šåŸºäºè®°å¿†IDå»é™¤é‡å¤é¡¹ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆé¡¹ç›®è®°å¿†ä¼˜å…ˆï¼‰
        seen_ids = set()
        unique_memories = []
        for memory in memories:
            if memory.id not in seen_ids:
                seen_ids.add(memory.id)
                unique_memories.append(memory)
        
        # åº”ç”¨è¿‡æ»¤å™¨
        filtered_memories = self._apply_memory_filters(unique_memories, config)
        
        # æŒ‰é‡è¦æ€§å’Œæ—¶é—´æ’åº
        filtered_memories.sort(
            key=lambda m: (m.importance, m.timestamp), 
            reverse=True
        )
        
        # é™åˆ¶æ•°é‡
        return filtered_memories[:config.max_memory_items]
    
    def _load_memories_from_path(self, base_path: Path, memory_types_to_load: List[MemoryType], source_label: str) -> List[MemoryEntry]:
        """ä»æŒ‡å®šè·¯å¾„åŠ è½½è®°å¿†"""
        memories = []
        
        # åŠ è½½å£°æ˜æ€§è®°å¿†
        if MemoryType.DECLARATIVE in memory_types_to_load:
            declarative_path = base_path / "memory" / "declarative.md"
            if declarative_path.exists():
                declarative_memories = self.markdown_engine.load_memories(declarative_path)
                for memory in declarative_memories:
                    memory.memory_type = "declarative"
                    memory.source = source_label  # æ ‡è®°è®°å¿†æ¥æº
                memories.extend(declarative_memories)
        
        # åŠ è½½ç¨‹åºæ€§è®°å¿†
        if MemoryType.PROCEDURAL in memory_types_to_load:
            procedural_path = base_path / "memory" / "procedural.md"
            if procedural_path.exists():
                # ä½¿ç”¨ä¸“é—¨çš„è§£æå™¨å¤„ç†procedural.mdæ ¼å¼
                try:
                    from .procedural_memory_parser import load_procedural_memories
                    memory_items = load_procedural_memories(procedural_path)
                    
                    # è½¬æ¢ä¸ºMemoryEntryæ ¼å¼
                    for memory_item in memory_items:
                        memory_entry = MemoryEntry(
                            id=memory_item.id,
                            timestamp=datetime.now().isoformat(),  # ä½¿ç”¨å½“å‰æ—¶é—´
                            content=memory_item.content,
                            tags=memory_item.tags,
                            project=memory_item.project,
                            importance=memory_item.importance,
                            source=source_label
                        )
                        memory_entry.memory_type = "procedural"
                        memories.append(memory_entry)
                    
                    if ENHANCED_SCORING_DEBUG:
                        print(f"ğŸ” ä½¿ç”¨ä¸“é—¨è§£æå™¨åŠ è½½procedural.md: {len(memory_items)} ä¸ªè®°å¿†æ¡ç›®")
                        
                except ImportError:
                    # å›é€€åˆ°åŸå§‹è§£æå™¨
                    if ENHANCED_SCORING_DEBUG:
                        print("âš ï¸ ä¸“é—¨è§£æå™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹è§£æå™¨")
                    procedural_memories = self.markdown_engine.load_memories(procedural_path)
                    for memory in procedural_memories:
                        memory.memory_type = "procedural"
                        memory.source = source_label  # æ ‡è®°è®°å¿†æ¥æº
                    memories.extend(procedural_memories)
        
        # åŠ è½½æƒ…æ™¯æ€§è®°å¿†
        if MemoryType.EPISODIC in memory_types_to_load:
            episodic_dir = base_path / "memory" / "episodic"
            if episodic_dir.exists():
                for episodic_file in episodic_dir.glob("*.md"):
                    episodic_memories = self.markdown_engine.load_memories(episodic_file)
                    for memory in episodic_memories:
                        memory.memory_type = "episodic"
                        memory.source = source_label  # æ ‡è®°è®°å¿†æ¥æº
                    memories.extend(episodic_memories)
        
        return memories
    
    def _apply_memory_filters(self, memories: List[MemoryEntry], config: ContextGenerationConfig) -> List[MemoryEntry]:
        """åº”ç”¨è®°å¿†è¿‡æ»¤å™¨"""
        filtered = memories
        
        # é‡è¦æ€§è¿‡æ»¤
        filtered = [m for m in filtered if m.importance >= config.memory_importance_threshold]
        
        # é¡¹ç›®èŒƒå›´è¿‡æ»¤
        if config.project_scope:
            filtered = [m for m in filtered if m.project == config.project_scope]
        
        # æ—¶é—´èŒƒå›´è¿‡æ»¤
        if config.time_range:
            start_time, end_time = config.time_range
            filtered = [
                m for m in filtered 
                if start_time <= m.timestamp <= end_time
            ]
        
        # æ ‡ç­¾è¿‡æ»¤
        if 'tags' in config.memory_filters:
            required_tags = config.memory_filters['tags']
            if isinstance(required_tags, str):
                required_tags = [required_tags]
            filtered = [
                m for m in filtered 
                if any(tag in m.tags for tag in required_tags)
            ]
        
        return filtered
    
    def _group_memories_by_type(self, memories: List[MemoryEntry]) -> Dict[str, List[MemoryEntry]]:
        """æŒ‰ç±»å‹åˆ†ç»„è®°å¿†"""
        groups = {
            "declarative": [],
            "procedural": [],
            "episodic": []
        }
        
        for memory in memories:
            memory_type = getattr(memory, 'memory_type', 'declarative')
            if memory_type in groups:
                groups[memory_type].append(memory)
            else:
                groups['declarative'].append(memory)  # é»˜è®¤å½’ç±»ä¸ºå£°æ˜æ€§è®°å¿†
        
        return groups
    
    def _load_framework_stage(self, stage: str) -> Optional[str]:
        """åŠ è½½æ¡†æ¶é˜¶æ®µå†…å®¹"""
        if stage not in self.stage_files:
            return None
        
        stage_file = self.framework_path / self.stage_files[stage]
        if stage_file.exists():
            return stage_file.read_text(encoding='utf-8')
        return None
    
    def _load_context_file(self, team_path: Path, stage: str, config: ContextGenerationConfig) -> Optional[str]:
        """åŠ è½½é¡¹ç›®æˆ–å›¢é˜Ÿç‰¹å®šçš„ä¸Šä¸‹æ–‡æ–‡ä»¶ï¼Œé¡¹ç›®ä¼˜å…ˆ"""
        content_parts = []
        
        # å¦‚æœæŒ‡å®šäº†é¡¹ç›®ï¼Œä¼˜å…ˆåŠ è½½é¡¹ç›®ä¸Šä¸‹æ–‡
        if config.project_name:
            project_path = team_path / "projects" / config.project_name
            project_context_file = project_path / "context" / f"{stage}.md"
            if project_context_file.exists():
                project_content = project_context_file.read_text(encoding='utf-8')
                filtered_content = self._filter_team_context_content(project_content)
                if filtered_content and filtered_content.strip():
                    content_parts.append(f"## é¡¹ç›®ä¸Šä¸‹æ–‡ ({config.project_name})")
                    content_parts.append(filtered_content)
        
        # å¦‚æœé…ç½®äº†åŒ…å«å›¢é˜Ÿä¸Šä¸‹æ–‡ï¼Œæˆ–è€…æ²¡æœ‰æŒ‡å®šé¡¹ç›®ï¼ŒåŠ è½½å›¢é˜Ÿä¸Šä¸‹æ–‡
        if config.include_team_memories or not config.project_name:
            team_context_file = team_path / "context" / f"{stage}.md"
            if team_context_file.exists():
                team_content = team_context_file.read_text(encoding='utf-8')
                filtered_content = self._filter_team_context_content(team_content)
                if filtered_content and filtered_content.strip():
                    if content_parts:  # å¦‚æœå·²æœ‰é¡¹ç›®ä¸Šä¸‹æ–‡ï¼Œæ·»åŠ åˆ†éš”
                        content_parts.append("## å›¢é˜Ÿä¸Šä¸‹æ–‡")
                    content_parts.append(filtered_content)
        
        return "\n\n".join(content_parts) if content_parts else None
    
    def _load_team_context_file(self, team_path: Path, stage: str) -> Optional[str]:
        """åŠ è½½å›¢é˜Ÿç‰¹å®šçš„ä¸Šä¸‹æ–‡æ–‡ä»¶ï¼Œè¿‡æ»¤æ‰å…ƒæ•°æ®éƒ¨åˆ†ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        context_file = team_path / "context" / f"{stage}.md"
        if context_file.exists():
            content = context_file.read_text(encoding='utf-8')
            return self._filter_team_context_content(content)
        return None
    
    def _filter_team_context_content(self, content: str) -> str:
        """è¿‡æ»¤å›¢é˜Ÿä¸Šä¸‹æ–‡å†…å®¹ï¼Œå»æ‰å…ƒæ•°æ®éƒ¨åˆ†ï¼Œåªä¿ç•™å®é™…å†…å®¹"""
        lines = content.split('\n')
        filtered_lines = []
        skip_metadata = False
        
        for line in lines:
            # è·³è¿‡å…ƒæ•°æ®ç›¸å…³çš„éƒ¨åˆ†
            if line.strip().startswith('## å…ƒæ•°æ®') or line.strip().startswith('## æœ€è¿‘æ›´æ–°'):
                skip_metadata = True
                continue
            
            # å¦‚æœé‡åˆ°æ–°çš„äºŒçº§æ ‡é¢˜ï¼Œä¸”ä¸æ˜¯å…ƒæ•°æ®ç›¸å…³ï¼Œåœæ­¢è·³è¿‡
            if line.strip().startswith('## ') and not any(keyword in line for keyword in ['å…ƒæ•°æ®', 'æœ€è¿‘æ›´æ–°']):
                skip_metadata = False
            
            # å¦‚æœé‡åˆ°ä¸‰çº§æ ‡é¢˜ï¼Œé€šå¸¸è¡¨ç¤ºå®é™…å†…å®¹å¼€å§‹
            if line.strip().startswith('### '):
                skip_metadata = False
            
            # å¦‚æœä¸åœ¨è·³è¿‡çŠ¶æ€ï¼Œä¸”ä¸æ˜¯å…ƒæ•°æ®è¡Œï¼Œåˆ™ä¿ç•™
            if not skip_metadata and not (line.strip().startswith('- **') and any(keyword in line for keyword in ['å›¢é˜Ÿ:', 'ç”Ÿæˆæ—¶é—´:', 'ä¸Šä¸‹æ–‡ç±»å‹:', 'æ—¶é—´:', 'è§¦å‘:'])):
                filtered_lines.append(line)
        
        # æ¸…ç†å¼€å¤´çš„ç©ºè¡Œå’Œæ ‡é¢˜
        while filtered_lines and (not filtered_lines[0].strip() or filtered_lines[0].strip().startswith('#')):
            filtered_lines.pop(0)
        
        return '\n'.join(filtered_lines).strip()
    
    def _load_team_context_files(self, team_path: Path, stages: List[str]) -> List[str]:
        """åŠ è½½å›¢é˜Ÿæ‰€æœ‰ä¸Šä¸‹æ–‡æ–‡ä»¶"""
        sections = []
        for stage in stages:
            content = self._load_team_context_file(team_path, stage)
            if content:
                sections.extend([
                    content,
                    "",
                    "---",
                    ""
                ])
        return sections
    
    def _find_memories_for_stage(self, memories: List[MemoryEntry], stage: str) -> List[MemoryEntry]:
        """ä¸ºç‰¹å®šé˜¶æ®µæ‰¾åˆ°ç›¸å…³è®°å¿†"""
        # å®šä¹‰æ¯ä¸ªé˜¶æ®µçš„å…³é”®è¯
        stage_keywords = {
            "requirements": ["éœ€æ±‚", "requirement", "éœ€è¦", "ç›®æ ‡", "goal", "objective"],
            "business-model": ["ä¸šåŠ¡", "business", "æ¨¡å‹", "model", "æµç¨‹", "process"],
            "solution": ["è§£å†³æ–¹æ¡ˆ", "solution", "æ–¹æ¡ˆ", "approach", "ç­–ç•¥", "strategy"],
            "structure": ["æ¶æ„", "architecture", "ç»“æ„", "structure", "è®¾è®¡", "design"],
            "tasks": ["ä»»åŠ¡", "task", "å·¥ä½œ", "work", "å®æ–½", "implementation"],
            "common-tasks": ["é€šç”¨", "common", "æ ‡å‡†", "standard", "æ¨¡æ¿", "template"],
            "constraints": ["çº¦æŸ", "constraint", "é™åˆ¶", "limitation", "è§„åˆ™", "rule"]
        }
        
        keywords = stage_keywords.get(stage, [])
        if not keywords:
            return []
        
        relevant_memories = []
        for memory in memories:
            # æ£€æŸ¥æ ‡ç­¾
            tag_match = any(
                any(keyword.lower() in tag.lower() for keyword in keywords)
                for tag in memory.tags
            )
            
            # æ£€æŸ¥å†…å®¹
            content_match = any(
                keyword.lower() in memory.content.lower() 
                for keyword in keywords
            )
            
            if tag_match or content_match:
                relevant_memories.append(memory)
        
        # æŒ‰é‡è¦æ€§æ’åº
        relevant_memories.sort(key=lambda m: m.importance, reverse=True)
        return relevant_memories
    
    def _find_relevant_memories_by_message(self, memories: List[MemoryEntry], user_message: str) -> List[MemoryEntry]:
        """æ ¹æ®ç”¨æˆ·æ¶ˆæ¯æ™ºèƒ½é€‰æ‹©ç›¸å…³è®°å¿†ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if not user_message or not memories:
            return []
        
        # æå–ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å…³é”®è¯
        message_keywords = self._extract_keywords_from_message(user_message.lower())
        
        # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œè¯´æ˜æ¶ˆæ¯ä¸æŠ€æœ¯å†…å®¹æ— å…³ï¼Œä¸åŠ è½½ä»»ä½•è®°å¿†
        if not message_keywords:
            return []
        
        # ä½¿ç”¨æ‰¹é‡è¯„åˆ†ä¼˜åŒ–
        if self.enable_optimized_scoring and self._optimized_scoring_engine:
            try:
                # æ‰¹é‡è®¡ç®—è¯„åˆ†
                batch_results = self._optimized_scoring_engine.batch_calculate_scores(
                    user_message.lower(), 
                    memories, 
                    max_workers=4
                )
                
                # è¿‡æ»¤å’Œæ’åº
                scored_memories = []
                for memory_id, score, details in batch_results:
                    if score >= 10.0:  # ç›¸å…³æ€§é˜ˆå€¼
                        # æ‰¾åˆ°å¯¹åº”çš„è®°å¿†å¯¹è±¡
                        memory = next((m for m in memories if m.id == memory_id), None)
                        if memory:
                            scored_memories.append((memory, score))
                
                if ENHANCED_SCORING_DEBUG and scored_memories:
                    print(f"ğŸš€ æ‰¹é‡è¯„åˆ†å®Œæˆ: {len(scored_memories)}/{len(memories)} ä¸ªè®°å¿†ç¬¦åˆé˜ˆå€¼")
                    # æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡
                    stats = self._optimized_scoring_engine.get_performance_stats()
                    print(f"   ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']}")
                    print(f"   å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}s")
                
            except Exception as e:
                if ENHANCED_SCORING_DEBUG:
                    print(f"âš ï¸ æ‰¹é‡è¯„åˆ†å¤±è´¥ï¼Œä½¿ç”¨å•ç‹¬è¯„åˆ†: {e}")
                # å›é€€åˆ°å•ç‹¬è¯„åˆ†
                scored_memories = self._calculate_individual_scores(memories, message_keywords, user_message.lower())
        else:
            # ä½¿ç”¨å•ç‹¬è¯„åˆ†
            scored_memories = self._calculate_individual_scores(memories, message_keywords, user_message.lower())
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿç›¸å…³çš„è®°å¿†ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if not scored_memories:
            return []
        
        # æŒ‰ç›¸å…³æ€§åˆ†æ•°å’Œé‡è¦æ€§æ’åº
        scored_memories.sort(key=lambda x: (x[1], x[0].importance), reverse=True)
        
        return [memory for memory, score in scored_memories]
    
    def _calculate_individual_scores(self, memories: List[MemoryEntry], message_keywords: List[str], user_message: str) -> List[Tuple[MemoryEntry, float]]:
        """å•ç‹¬è®¡ç®—æ¯ä¸ªè®°å¿†çš„è¯„åˆ†ï¼ˆåŸå§‹æ–¹æ³•ï¼‰"""
        scored_memories = []
        for memory in memories:
            score = self._calculate_memory_relevance_score(memory, message_keywords, user_message)
            # è°ƒæ•´ç›¸å…³æ€§é˜ˆå€¼ï¼šå¢å¼ºè¯„åˆ†å¼•æ“çš„åˆ†æ•°èŒƒå›´é€šå¸¸æ›´é«˜
            # é™ä½é˜ˆå€¼ä»¥é€‚åº”æ–°çš„è¯„åˆ†ç³»ç»Ÿ
            if score >= 10.0:
                scored_memories.append((memory, score))
        return scored_memories
    
    def _extract_keywords_from_message(self, message: str) -> List[str]:
        """ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–å…³é”®è¯"""
        import re
        
        # è¿‡æ»¤åœç”¨è¯ - æ‰©å±•åˆ—è¡¨ï¼Œè¿‡æ»¤æ›´å¤šæ— å…³è¯æ±‡
        stop_words = {
            # è‹±æ–‡åœç”¨è¯
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',
            'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those',
            'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his',
            'get', 'make', 'go', 'come', 'know', 'think', 'see', 'want', 'use', 'find', 'give', 'tell', 'work',
            'call', 'try', 'ask', 'need', 'feel', 'become', 'leave', 'put', 'mean', 'keep', 'let', 'begin',
            # ä¸­æ–‡åœç”¨è¯
            'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬', 'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'è¿™', 'é‚£',
            'ä¸€ä¸ª', 'è¯·', 'å¸®', 'æˆ‘è¦', 'éœ€è¦', 'å¸Œæœ›', 'å¯ä»¥', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å› ä¸º',
            'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å', 'ç°åœ¨', 'å·²ç»', 'è¿˜æ˜¯', 'å°±æ˜¯', 'éƒ½æ˜¯', 'ä¸æ˜¯', 'æ²¡æœ‰', 'ä¹Ÿæ˜¯', 'æˆ–è€…',
            'å…¶ä»–', 'å…¶å®ƒ', 'ä¸€äº›', 'å¾ˆå¤š', 'éå¸¸', 'ç‰¹åˆ«', 'æ¯”è¾ƒ', 'è§‰å¾—', 'åº”è¯¥', 'å¯èƒ½', 'ä¸€ç›´', 'æ€»æ˜¯',
            'ä»æ¥', 'ä»ä¸', 'æ°¸è¿œ', 'é©¬ä¸Š', 'ç«‹å³', 'ç°åœ¨', 'ä»¥å‰', 'ä»¥å', 'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©'
        }
        
        keywords = []
        
        # å¤„ç†è‹±æ–‡è¯æ±‡ï¼ˆæŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        english_words = re.findall(r'[a-zA-Z]+', message)
        for word in english_words:
            word_lower = word.lower()
            # æé«˜è‹±æ–‡å•è¯çš„æœ€å°é•¿åº¦è¦æ±‚ï¼Œé¿å…æå–æ— æ„ä¹‰çš„çŸ­è¯
            if len(word) >= 3 and word_lower not in stop_words:
                keywords.append(word_lower)
        
        # å¤„ç†ä¸­æ–‡å…³é”®è¯ï¼ˆä½¿ç”¨ç®€å•çš„è§„åˆ™è¯†åˆ«ï¼‰
        chinese_text = re.sub(r'[^\u4e00-\u9fa5]', '', message)
        
        # è¯†åˆ«å¸¸è§çš„æŠ€æœ¯æœ¯è¯­å’Œæ¦‚å¿µ - æ›´ç²¾å‡†çš„åŒ¹é…
        tech_patterns = [
            r'å·¥ä½œæµ', r'workflow', r'API', r'api', r'æ¥å£', r'æ•°æ®åº“', r'database', 
            r'è®¤è¯', r'authentication', r'æƒé™', r'authorization', r'ç®¡ç†', r'management',
            r'æœåŠ¡', r'service', r'æŸ¥è¯¢', r'query', r'åˆ†é¡µ', r'pagination', 
            r'æ¶æ„', r'architecture', r'å®ç°', r'implementation', r'é…ç½®', r'configuration',
            r'æ¡†æ¶', r'framework', r'æ¨¡å‹', r'model', r'ä¸šåŠ¡', r'business', 
            r'æµç¨‹', r'process', r'åŠŸèƒ½', r'feature', r'æ¨¡å—', r'module', r'ç»„ä»¶', r'component',
            r'Rule', r'rule', r'Solution', r'solution', r'Prompt', r'prompt'
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, message, re.IGNORECASE)
            for match in matches:
                if match.lower() not in [kw.lower() for kw in keywords]:
                    keywords.append(match.lower())
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…³é”®è¯ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦åˆ‡åˆ†ä½œä¸ºå¤‡é€‰
        if not keywords and chinese_text:
            # ç®€å•çš„ä¸­æ–‡åŒå­—è¯æå–
            for i in range(len(chinese_text) - 1):
                two_char = chinese_text[i:i+2]
                if two_char not in stop_words:
                    keywords.append(two_char)
        
        return list(set(keywords))  # å»é‡
    
    def _calculate_memory_relevance_score(self, memory: MemoryEntry, message_keywords: List[str], full_message: str) -> float:
        """è®¡ç®—è®°å¿†ä¸ç”¨æˆ·æ¶ˆæ¯çš„ç›¸å…³æ€§åˆ†æ•°ï¼ˆé›†æˆä¼˜åŒ–è¯„åˆ†ç®—æ³•ï¼‰"""
        
        # ä¼˜å…ˆä½¿ç”¨ä¼˜åŒ–è¯„åˆ†å¼•æ“
        if self.enable_optimized_scoring and self._optimized_scoring_engine:
            try:
                score, details = self._optimized_scoring_engine.calculate_memory_score(full_message, memory)
                
                if ENHANCED_SCORING_DEBUG:
                    print(f"ğŸš€ ä¼˜åŒ–è¯„åˆ† - {memory.id}: {score:.2f}")
                    print(f"   åŒ¹é…å…³é”®è¯: {', '.join(details.get('matched_keywords', [])[:5])}")
                    print(f"   å…³é”®ä¼˜åŠ¿: {', '.join(details.get('key_strengths', [])[:3])}")
                    print(f"   ç¼“å­˜çŠ¶æ€: {'å‘½ä¸­' if details.get('cached') else 'æœªå‘½ä¸­'}")
                
                return score
                
            except Exception as e:
                if ENHANCED_SCORING_DEBUG:
                    print(f"âš ï¸ ä¼˜åŒ–è¯„åˆ†å¼•æ“å‡ºé”™ï¼Œå›é€€åˆ°å¢å¼ºç®—æ³•: {e}")
        
        # å›é€€åˆ°å¢å¼ºè¯„åˆ†ç®—æ³•
        if ENABLE_ENHANCED_SCORING:
            try:
                # å°è¯•ä½¿ç”¨å¢å¼ºçš„è¯„åˆ†å¼•æ“
                from src.scoring_self_evolution import SelfLearningMemoryScoringEngine, MemoryItem
                
                # å°†MemoryEntryè½¬æ¢ä¸ºMemoryItem
                memory_item = MemoryItem(
                    id=memory.id,
                    title=getattr(memory, 'title', memory.id),
                    content=memory.content,
                    tags=memory.tags,
                    project=memory.project,
                    importance=memory.importance
                )
                
                # åˆ›å»ºå¢å¼ºè¯„åˆ†å¼•æ“
                scoring_engine = SelfLearningMemoryScoringEngine()
                
                # ä½¿ç”¨å¢å¼ºè¯„åˆ†ç®—æ³•
                results = scoring_engine.score_memory_items(full_message, [memory_item])
                
                if results:
                    enhanced_score = results[0].total_score
                    
                    if ENHANCED_SCORING_DEBUG:
                        print(f"ğŸ” å¢å¼ºè¯„åˆ† - {memory.id}: {enhanced_score:.2f}")
                        print(f"   åŒ¹é…å…³é”®è¯: {', '.join(results[0].matched_keywords[:5])}")
                        print(f"   å…³é”®ä¼˜åŠ¿: {', '.join(results[0].key_strengths[:3])}")
                    
                    # è¿”å›å¢å¼ºè¯„åˆ†ç»“æœ
                    return enhanced_score
                    
            except ImportError:
                # å¦‚æœå¢å¼ºè¯„åˆ†å¼•æ“ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸå§‹ç®—æ³•
                if ENHANCED_SCORING_DEBUG:
                    print("âš ï¸ å¢å¼ºè¯„åˆ†å¼•æ“ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸå§‹ç®—æ³•")
            except Exception as e:
                # å¦‚æœå¢å¼ºè¯„åˆ†å‡ºç°é”™è¯¯ï¼Œå›é€€åˆ°åŸå§‹ç®—æ³•
                if ENHANCED_SCORING_DEBUG:
                    print(f"âš ï¸ å¢å¼ºè¯„åˆ†ç®—æ³•å‡ºé”™ï¼Œå›é€€åˆ°åŸå§‹ç®—æ³•: {e}")
        
        # åŸå§‹è¯„åˆ†ç®—æ³•ï¼ˆä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰
        score = 0.0
        
        # 1. æ ‡ç­¾åŒ¹é… (æƒé‡: 3.0)
        for tag in memory.tags:
            tag_lower = tag.lower()
            for keyword in message_keywords:
                if keyword in tag_lower or tag_lower in keyword:
                    score += 3.0
        
        # 2. å†…å®¹å…³é”®è¯åŒ¹é… (æƒé‡: 2.0)
        memory_content_lower = memory.content.lower()
        for keyword in message_keywords:
            if keyword in memory_content_lower:
                score += 2.0
        
        # 3. é¡¹ç›®ååŒ¹é… (æƒé‡: 1.5)
        if memory.project and memory.project.lower() != 'general':
            project_lower = memory.project.lower()
            for keyword in message_keywords:
                if keyword in project_lower or project_lower in keyword:
                    score += 1.5
        
        # 4. å®Œæ•´çŸ­è¯­åŒ¹é… (æƒé‡: 4.0)
        # å¯»æ‰¾ç”¨æˆ·æ¶ˆæ¯ä¸­çš„2-3è¯ç»„åˆæ˜¯å¦åœ¨è®°å¿†å†…å®¹ä¸­å‡ºç°
        for i in range(len(message_keywords) - 1):
            phrase = " ".join(message_keywords[i:i+2])
            if phrase in memory_content_lower:
                score += 4.0
        
        # 5. è¯­ä¹‰ç›¸å…³æ€§åŒ¹é… (æƒé‡: 1.5å€ï¼Œå› ä¸ºè¯­ä¹‰æ¯”å­—é¢åŒ¹é…æ›´é‡è¦)
        semantic_score = self._calculate_semantic_relevance(memory, message_keywords, full_message)
        score += semantic_score * 1.5  # æé«˜è¯­ä¹‰ç›¸å…³æ€§çš„æƒé‡
        
        # 6. é‡è¦æ€§åŠ æƒ
        score *= (memory.importance / 3.0)  # é‡è¦æ€§å½’ä¸€åŒ–åˆ°ç›¸å¯¹æƒé‡
        
        return score
    
    def _calculate_semantic_relevance(self, memory: MemoryEntry, message_keywords: List[str], full_message: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸å…³æ€§å¾—åˆ† - åŸºäºé€šç”¨è¯­ä¹‰åŒ¹é…åŸåˆ™"""
        semantic_score = 0.0
        full_message_lower = full_message.lower()
        memory_content_lower = memory.content.lower()
        memory_tags_lower = ' '.join(memory.tags).lower()
        memory_text = memory_content_lower + ' ' + memory_tags_lower
        
        # 1. é¢†åŸŸæ¦‚å¿µå¯†åº¦è¯„åˆ† (0-10åˆ†)
        # è®¡ç®—ç”¨æˆ·æ¶ˆæ¯å’Œè®°å¿†å†…å®¹ä¸­å…±åŒæŠ€æœ¯æ¦‚å¿µçš„å¯†åº¦
        domain_keywords = ['api', 'workflow', 'solution', 'rule', 'step', 'validation', 'model', 
                          'architecture', 'design', 'service', 'id', 'reference', 'create', 'update']
        
        user_domain_concepts = [kw for kw in message_keywords if kw in domain_keywords]
        memory_domain_matches = sum(1 for concept in user_domain_concepts if concept in memory_text)
        
        if user_domain_concepts:
            domain_density = (memory_domain_matches / len(user_domain_concepts)) * 10
            semantic_score += domain_density
        
        # 2. é—®é¢˜-è§£å†³æ–¹æ¡ˆåŒ¹é…åº¦ (0-15åˆ†)
        # æ£€æµ‹ç”¨æˆ·æ¶ˆæ¯ä¸­çš„é—®é¢˜ç±»å‹ï¼Œè¯„ä¼°è®°å¿†æ˜¯å¦æä¾›ç›¸åº”è§£å†³æ–¹æ¡ˆ
        problem_solution_pairs = [
            (['enhance', 'improve', 'add', 'support'], ['design', 'architecture', 'implementation', 'approach']),
            (['validate', 'check', 'ensure'], ['validation', 'verification', 'logic', 'mechanism']),
            (['reference', 'link', 'connect'], ['relationship', 'mapping', 'association', 'routing']),
            (['create', 'build', 'generate'], ['creation', 'construction', 'generation', 'workflow']),
            (['model', 'structure', 'entity'], ['class', 'inheritance', 'hierarchy', 'design'])
        ]
        
        for problem_words, solution_words in problem_solution_pairs:
            has_problem = any(word in message_keywords for word in problem_words)
            has_solution = any(word in memory_text for word in solution_words)
            if has_problem and has_solution:
                semantic_score += 3.0  # æ¯ä¸ªé—®é¢˜-è§£å†³æ–¹æ¡ˆåŒ¹é…å¾—3åˆ†
        
        # 3. å¤åˆæ¦‚å¿µåŒ¹é… (0-20åˆ†)
        # è¯†åˆ«ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å¤åˆæ¦‚å¿µï¼Œå¹¶åœ¨è®°å¿†ä¸­å¯»æ‰¾è¯­ä¹‰ç›¸å…³çš„è§£å†³æ–¹æ¡ˆ
        import re
        
        # æå–ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å…³é”®çŸ­è¯­
        user_phrases = re.findall(r'[a-z]+(?:\s+[a-z]+){1,2}', full_message_lower)
        
        # å®šä¹‰å¤åˆæ¦‚å¿µçš„è¯­ä¹‰æ˜ å°„
        concept_mappings = [
            # Solution as step æ ¸å¿ƒæ¦‚å¿µ
            (['solution as step', 'solution.*step', 'setting solution.*step'], 
             ['orderedsteps', 'step.*solution', 'solution.*workflow', 'list.*string']),
            
            # Reference and ID concepts  
            (['solution reference', 'solution.*id', 'reference.*solution'], 
             ['id.*prefix', 'prefix.*identification', 's_.*uuid', 'service.*routing']),
            
            # Validation concepts
            (['validate.*solution', 'ensure.*valid', 'validate.*exist'],
             ['validation.*logic', 'exist.*rule', 'prompt.*exist', 'routing.*service']),
            
            # Workflow creation concepts
            (['workflow creation', 'creating workflow', 'workflow.*api'],
             ['create.*workflow', 'workflow.*design', 'api.*design', 'controller.*service']),
            
            # Data model concepts
            (['data model', 'dto.*entit', 'model.*support'],
             ['inherit.*relation', 'class.*design', 'architecture.*design', 'prompt.*base'])
        ]
        
        for user_patterns, memory_patterns in concept_mappings:
            user_match = False
            memory_match = False
            
            # æ£€æŸ¥ç”¨æˆ·æ¶ˆæ¯ä¸­çš„æ¦‚å¿µ
            for pattern in user_patterns:
                if re.search(pattern, full_message_lower) or any(pattern.replace('.*', ' ') in phrase for phrase in user_phrases):
                    user_match = True
                    break
            
            # æ£€æŸ¥è®°å¿†ä¸­çš„ç›¸å…³è§£å†³æ–¹æ¡ˆ
            for pattern in memory_patterns:
                if re.search(pattern, memory_text):
                    memory_match = True
                    break
            
            # å¤åˆæ¦‚å¿µåŒ¹é…ç»™äºˆæ›´é«˜åˆ†æ•°
            if user_match and memory_match:
                semantic_score += 4.0  # æ¯ä¸ªå¤åˆæ¦‚å¿µåŒ¹é…å¾—4åˆ†
        
        # 4. æŠ€æœ¯æ ˆç›¸å…³æ€§ (0-5åˆ†)
        # æ£€æŸ¥æŠ€æœ¯æ ˆçš„åŒ¹é…åº¦
        tech_stack_keywords = ['dto', 'entity', 'controller', 'service', 'repository', 'database',
                              'validation', 'routing', 'prefix', 'inheritance', 'polymorphism']
        
        tech_matches = sum(1 for tech in tech_stack_keywords 
                          if tech in full_message_lower and tech in memory_text)
        semantic_score += min(5, tech_matches)
        
        return semantic_score
    
    def get_scoring_performance_stats(self) -> Dict:
        """è·å–è¯„åˆ†å¼•æ“æ€§èƒ½ç»Ÿè®¡"""
        if self.enable_optimized_scoring and self._optimized_scoring_engine:
            return self._optimized_scoring_engine.get_performance_stats()
        return {'message': 'ä¼˜åŒ–è¯„åˆ†å¼•æ“æœªå¯ç”¨'}
    
    def save_scoring_engine_state(self):
        """ä¿å­˜è¯„åˆ†å¼•æ“çŠ¶æ€"""
        if self.enable_optimized_scoring and self._optimized_scoring_engine:
            self._optimized_scoring_engine.save_state()
            print("âœ… è¯„åˆ†å¼•æ“çŠ¶æ€å·²ä¿å­˜")
    
    def update_keyword_weight(self, keyword: str, dimension: str, weight: float, confidence: float = 0.8):
        """æ›´æ–°å…³é”®è¯æƒé‡"""
        if self.enable_optimized_scoring and self._optimized_scoring_engine:
            self._optimized_scoring_engine.update_keyword_weight(keyword, dimension, weight, confidence)
            print(f"âœ… å…³é”®è¯æƒé‡å·²æ›´æ–°: {keyword} ({dimension}) = {weight}")
        else:
            print("âš ï¸ ä¼˜åŒ–è¯„åˆ†å¼•æ“æœªå¯ç”¨ï¼Œæ— æ³•æ›´æ–°æƒé‡")
    
    def _get_unmatched_memories(self, memories: List[MemoryEntry], stages: List[str]) -> List[MemoryEntry]:
        """è·å–æœªåŒ¹é…åˆ°ä»»ä½•é˜¶æ®µçš„è®°å¿†"""
        matched_memory_ids = set()
        
        # æ”¶é›†æ‰€æœ‰å·²åŒ¹é…çš„è®°å¿†ID
        for stage in stages:
            stage_memories = self._find_memories_for_stage(memories, stage)
            matched_memory_ids.update(m.id for m in stage_memories)
        
        # è¿”å›æœªåŒ¹é…çš„è®°å¿†
        unmatched = [m for m in memories if m.id not in matched_memory_ids]
        return sorted(unmatched, key=lambda m: m.importance, reverse=True)


# ä¾¿æ·å‡½æ•°
def create_memory_only_config(team_name: str, project_name: str = None, **kwargs) -> ContextGenerationConfig:
    """åˆ›å»ºä»…è®°å¿†æ¨¡å¼çš„é…ç½®"""
    return ContextGenerationConfig(
        team_name=team_name,
        project_name=project_name,
        mode=ContextMode.MEMORY_ONLY,
        **kwargs
    )


def create_framework_only_config(team_name: str, project_name: str = None, stages: List[str] = None, **kwargs) -> ContextGenerationConfig:
    """åˆ›å»ºä»…æ¡†æ¶æ¨¡å¼çš„é…ç½®"""
    if stages is None:
        stages = ["requirements", "business-model", "solution", "structure", "tasks", "common-tasks", "constraints"]
    
    return ContextGenerationConfig(
        team_name=team_name,
        project_name=project_name,
        mode=ContextMode.FRAMEWORK_ONLY,
        include_framework_stages=stages,
        **kwargs
    )


def create_hybrid_config(team_name: str, project_name: str = None, stages: List[str] = None, **kwargs) -> ContextGenerationConfig:
    """åˆ›å»ºæ··åˆæ¨¡å¼çš„é…ç½®"""
    if stages is None:
        stages = ["requirements", "business-model", "solution", "structure", "tasks", "common-tasks", "constraints"]
    
    return ContextGenerationConfig(
        team_name=team_name,
        project_name=project_name,
        mode=ContextMode.HYBRID,
        include_framework_stages=stages,
        **kwargs
    ) 