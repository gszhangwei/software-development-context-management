#!/usr/bin/env python3
"""
AIæ¨¡å‹é›†æˆè¿è¡Œå™¨

æ•´åˆå¤šAIæ¨¡å‹åˆ›å»ºã€ä½¿ç”¨å’Œå­˜å‚¨åŠŸèƒ½çš„ä¸»è°ƒç”¨æ–‡ä»¶
æ”¯æŒClaudeã€OpenAIç­‰å¤šç§AIæ¨¡å‹æä¾›å•†
"""

import sys
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from .ai_model_factory import create_ai_model
from .model_usage_manager import create_model_usage_manager
from .model_storage_manager import create_claude_storage


class ModelRunner:
    """AIæ¨¡å‹è¿è¡Œå™¨ä¸»ç±»"""
    
    def __init__(self, model_name="claude-sonnet-4-20250514", 
                 team_data_root="test_data", output_dir="output"):
        """
        åˆå§‹åŒ–AIæ¨¡å‹è¿è¡Œå™¨
        
        Args:
            model_name: AIæ¨¡å‹åç§°ï¼ˆæ”¯æŒClaudeå’ŒOpenAIæ¨¡å‹ï¼‰
            team_data_root: å›¢é˜Ÿæ•°æ®æ ¹ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_name = model_name
        self.team_data_root = team_data_root
        self.output_dir = output_dir
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.ai_model = create_ai_model(model_name)
        self.model_usage = create_model_usage_manager(model_name, team_data_root)
        self.model_storage = create_claude_storage(output_dir)
    
    def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•AIæ¨¡å‹APIè¿æ¥"""
        print("ğŸ§ª æµ‹è¯•AIæ¨¡å‹APIåŸºæœ¬è¿æ¥")
        print("=" * 40)
        
        result = self.ai_model.test_connection()
        return result
    
    def test_team_context_generation(self) -> Dict[str, Any]:
        """æµ‹è¯•å›¢é˜Ÿä¸Šä¸‹æ–‡ç”Ÿæˆ"""
        print("\nğŸ§ª æµ‹è¯•å›¢é˜Ÿä¸Šä¸‹æ–‡ç”Ÿæˆ")
        print("=" * 40)
        
        result = self.model_usage.test_team_context_generation()
        return result
    
    def run_with_context(self, user_message: str, team_name: str, 
                        mode: str = "framework_only", max_tokens: int = None,
                        temperature: float = 0.5, save_results: bool = True) -> Dict[str, Any]:
        """
        ä½¿ç”¨å›¢é˜Ÿä¸Šä¸‹æ–‡è¿è¡ŒAIæ¨¡å‹
        
        Args:
            user_message: ç”¨æˆ·æ¶ˆæ¯
            team_name: å›¢é˜Ÿåç§°
            mode: ä¸Šä¸‹æ–‡æ¨¡å¼
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        
        Returns:
            è¿è¡Œç»“æœ
        """
        print(f"\nğŸ§ª æµ‹è¯•AIæ¨¡å‹ + å›¢é˜Ÿä¸Šä¸‹æ–‡é›†æˆ")
        print("=" * 40)
        print(f"ğŸ”„ ä½¿ç”¨å›¢é˜Ÿ '{team_name}' æµ‹è¯•é›†æˆ...")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨è®¾ç½®åˆé€‚çš„max_tokens
        if max_tokens is None:
            from .ai_model_factory import MODEL_CONFIGS
            model_config = MODEL_CONFIGS.get(self.model_name, {})
            provider = model_config.get("provider", "unknown")
            
            if provider == "openai":
                # OpenAIæ¨¡å‹é€šå¸¸æ”¯æŒè¾ƒå°‘çš„è¾“å‡ºtoken
                max_tokens = 16000  # ä¸ºOpenAIæ¨¡å‹è®¾ç½®è¾ƒå°çš„å€¼
            else:
                # Claudeç­‰å…¶ä»–æ¨¡å‹å¯ä»¥ä½¿ç”¨æ›´å¤§çš„å€¼
                max_tokens = 20000
        
        print(f"ğŸ“Š ä½¿ç”¨å‚æ•°: max_tokens={max_tokens}, temperature={temperature}")
        
        # è°ƒç”¨AIæ¨¡å‹API
        result = self.model_usage.chat_with_context(
            user_message=user_message,
            team_name=team_name,
            mode=mode,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        if not result["success"]:
            print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {result['error']}")
            return result
        
        # æ˜¾ç¤ºç»“æœç»Ÿè®¡
        print(f"âœ… é›†æˆæµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“Š ç»“æœç»Ÿè®¡:")
        print(f"   - ç³»ç»Ÿæç¤ºè¯é•¿åº¦: {result['system_prompt_length']}å­—ç¬¦")
        print(f"   - ç”¨æˆ·æ¶ˆæ¯é•¿åº¦: {result['user_message_length']}å­—ç¬¦")
        print(f"   - å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
        print(f"   - è¾“å…¥ä»¤ç‰Œ: {result['input_tokens']}")
        print(f"   - è¾“å‡ºä»¤ç‰Œ: {result['output_tokens']}")
        print(f"   - æ€»ä»¤ç‰Œ: {result['total_tokens']}")
        print(f"   - å“åº”é•¿åº¦: {result['response_length']}å­—ç¬¦")
        
        # æ˜¾ç¤ºå“åº”é¢„è§ˆ
        print(f"\nğŸ“„ AIå“åº”é¢„è§ˆ:")
        print("-" * 50)
        response_preview = result["response"][:300] if result["response"] else ""
        print(f"{response_preview}{'...' if len(result['response']) > 300 else ''}")
        print("-" * 50)
        
        # ä¿å­˜ç»“æœï¼ˆå¦‚æœéœ€è¦ï¼‰
        if save_results:
            try:
                saved_paths = self.model_storage.save_complete_result(result)
                print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
                print(f"   - ç³»ç»Ÿæç¤ºè¯: {saved_paths['system_prompt']}")
                print(f"   - AIå“åº”: {saved_paths['response']}")
                print(f"   - å…ƒæ•°æ®: {saved_paths['metadata']}")
                
                result["saved_paths"] = saved_paths
                
            except Exception as e:
                print(f"âš ï¸  ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}")
                result["save_error"] = str(e)
        
        return result
    
    def run_comprehensive_test(self, user_message: str = None, context_mode: str = "framework_only") -> Dict[str, Any]:
        """
        è¿è¡Œç»¼åˆæµ‹è¯•
        
        Args:
            user_message: å¯é€‰çš„ç”¨æˆ·æ¶ˆæ¯
            context_mode: ä¸Šä¸‹æ–‡æ¨¡å¼ (framework_only, memory_only, hybrid)
        
        Returns:
            ç»¼åˆæµ‹è¯•ç»“æœ
        """
        print(f"ğŸ¤– AIæ¨¡å‹ç»¼åˆæµ‹è¯•")
        print("=" * 60)
        
        results = {
            "connection_test": None,
            "context_test": None,
            "integration_test": None,
            "success_count": 0,
            "total_tests": 3,
            "context_mode": context_mode
        }
        
        # 1. è¿æ¥æµ‹è¯•
        connection_result = self.test_connection()
        results["connection_test"] = connection_result
        if connection_result.get("success", False):
            results["success_count"] += 1
        
        # 2. ä¸Šä¸‹æ–‡ç”Ÿæˆæµ‹è¯•
        context_result = self.test_team_context_generation()
        results["context_test"] = context_result
        if context_result.get("success", False):
            results["success_count"] += 1
        
        # 3. é›†æˆæµ‹è¯•ï¼ˆå¦‚æœå‰é¢æµ‹è¯•æˆåŠŸï¼‰
        if results["success_count"] >= 1:  # è‡³å°‘ä¸Šä¸‹æ–‡æµ‹è¯•æˆåŠŸ
            if user_message is None:
                user_message = "è¯·å¸®æˆ‘è®¾è®¡ä¸€ä¸ªé«˜å¯ç”¨æ€§çš„å¾®æœåŠ¡æ¶æ„ï¼ŒåŒ…æ‹¬APIç½‘å…³ã€æœåŠ¡å‘ç°ã€è´Ÿè½½å‡è¡¡å’Œç›‘æ§ç»„ä»¶ã€‚"
            
            available_teams = context_result.get("available_teams", [])
            if available_teams:
                test_team = available_teams[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨å›¢é˜Ÿ
                integration_result = self.run_with_context(
                    user_message=user_message,
                    team_name=test_team,
                    mode=context_mode  # ä½¿ç”¨ä¼ å…¥çš„ä¸Šä¸‹æ–‡æ¨¡å¼
                )
                results["integration_test"] = integration_result
                if integration_result.get("success", False):
                    results["success_count"] += 1
        
        # æ˜¾ç¤ºç»¼åˆç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 40)
        print(f"åŸºæœ¬è¿æ¥æµ‹è¯•: {'âœ… é€šè¿‡' if results['connection_test'].get('success') else 'âŒ å¤±è´¥'}")
        print(f"å›¢é˜Ÿä¸Šä¸‹æ–‡ç”Ÿæˆæµ‹è¯•: {'âœ… é€šè¿‡' if results['context_test'].get('success') else 'âŒ å¤±è´¥'}")
        print(f"AIæ¨¡å‹ + ä¸Šä¸‹æ–‡é›†æˆæµ‹è¯•: {'âœ… é€šè¿‡' if results['integration_test'] and results['integration_test'].get('success') else 'âŒ å¤±è´¥'}")
        
        print(f"\næ€»è®¡: {results['success_count']}/{results['total_tests']} ä¸ªæµ‹è¯•é€šè¿‡")
        
        # æ˜¾ç¤ºå­˜å‚¨ä¿¡æ¯
        storage_info = self.model_storage.get_storage_info()
        print(f"\nğŸ“ å­˜å‚¨ä¿¡æ¯:")
        print(f"   - ç³»ç»Ÿæç¤ºè¯: {storage_info['system_prompts_count']}ä¸ªæ–‡ä»¶")
        print(f"   - AIå“åº”: {storage_info['responses_count']}ä¸ªæ–‡ä»¶")
        print(f"   - å…ƒæ•°æ®: {storage_info['metadata_count']}ä¸ªæ–‡ä»¶")
        
        # æœ€ç»ˆçŠ¶æ€
        if results["success_count"] == results["total_tests"]:
            print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIæ¨¡å‹é›†æˆæ­£å¸¸å·¥ä½œã€‚")
        elif results["success_count"] > 0:
            print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç½‘ç»œè¿æ¥ã€‚")
        else:
            print(f"\nâŒ æ‰€æœ‰æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€APIå¯†é’¥å’Œç½‘ç»œè¿æ¥ã€‚")
        
        results["overall_success"] = results["success_count"] == results["total_tests"]
        return results
    
    def get_runner_info(self) -> Dict[str, Any]:
        """è·å–è¿è¡Œå™¨ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "team_data_root": self.team_data_root,
            "output_dir": self.output_dir,
            "model_info": self.ai_model.get_client_info(),
            "usage_info": {
                "available_teams": self.model_usage.get_available_teams()
            },
            "storage_info": self.model_storage.get_storage_info()
        }


def create_model_runner(model_name="claude-sonnet-4-20250514", 
                       team_data_root="test_data", output_dir="output") -> ModelRunner:
    """
    ä¾¿æ·å‡½æ•°ï¼šåˆ›å»ºAIæ¨¡å‹è¿è¡Œå™¨
    
    Args:
        model_name: AIæ¨¡å‹åç§°
        team_data_root: å›¢é˜Ÿæ•°æ®æ ¹ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
    
    Returns:
        ModelRunnerå®ä¾‹
    """
    return ModelRunner(model_name=model_name, team_data_root=team_data_root, output_dir=output_dir)


# å‘åå…¼å®¹çš„åˆ«å
def create_claude_runner(model_name="claude-sonnet-4-20250514", 
                        team_data_root="test_data", output_dir="output") -> ModelRunner:
    """å‘åå…¼å®¹ï¼šåˆ›å»ºClaudeè¿è¡Œå™¨"""
    return create_model_runner(model_name=model_name, team_data_root=team_data_root, output_dir=output_dir)


# ä¿æŒå‘åå…¼å®¹çš„ç±»åˆ«å
ClaudeRunner = ModelRunner


def main(user_message: str = None):
    """
    ä¸»å‡½æ•°
    
    Args:
        user_message: å¯é€‰çš„è‡ªå®šä¹‰ç”¨æˆ·æ¶ˆæ¯
    """
    try:
        # åˆ›å»ºè¿è¡Œå™¨
        runner = create_model_runner()
        
        # è¿è¡Œç»¼åˆæµ‹è¯•
        result = runner.run_comprehensive_test(user_message)
        
        return result
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå™¨æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


if __name__ == "__main__":
    main() 