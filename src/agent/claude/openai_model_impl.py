"""
OpenAIæ¨¡å‹å®ç°æ¨¡å—

å®ç°äº†åŸºäºOpenAI APIçš„AIæ¨¡å‹
"""

import sys
import time
from pathlib import Path
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# æ£€æŸ¥ä¾èµ–åŒ…
try:
    import openai
    HAS_OPENAI = True
except ImportError:
    HAS_OPENAI = False

from src.agent.env_config import get_env_config
from .ai_model_base import AIModelBase


class OpenAIModel(AIModelBase):
    """OpenAIæ¨¡å‹å®ç°ç±»"""
    
    def _get_provider(self) -> str:
        return "openai"
    
    def _get_token_param_name(self) -> str:
        """è·å–å½“å‰æ¨¡å‹åº”è¯¥ä½¿ç”¨çš„tokenå‚æ•°åç§°"""
        from .ai_model_factory import MODEL_CONFIGS
        model_config = MODEL_CONFIGS.get(self.model_name, {})
        return "max_completion_tokens" if model_config.get("use_completion_tokens", False) else "max_tokens"
    
    def _should_use_default_temperature(self) -> bool:
        """æ£€æŸ¥å½“å‰æ¨¡å‹æ˜¯å¦åº”è¯¥ä½¿ç”¨é»˜è®¤æ¸©åº¦"""
        from .ai_model_factory import MODEL_CONFIGS
        model_config = MODEL_CONFIGS.get(self.model_name, {})
        return model_config.get("force_default_temperature", False)
    
    def _initialize_client(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        if not HAS_OPENAI:
            raise ImportError("éœ€è¦å®‰è£…openaiåº“: pip install openai")
        
        # è·å–APIå¯†é’¥
        env_config = get_env_config()
        self.api_key = env_config.openai_api_key
        
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ°OPENAI_API_KEYï¼Œè¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®")
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        self.client = openai.OpenAI(api_key=self.api_key)
    
    def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•OpenAI APIè¿æ¥"""
        try:
            print(f"âœ… OpenAI APIå¯†é’¥å·²é…ç½®: {self.api_key[:8]}...{self.api_key[-4:]}")
            print("âœ… OpenAIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            
            # æµ‹è¯•ç®€å•çš„APIè°ƒç”¨
            print("ğŸ”„ æµ‹è¯•OpenAI APIè°ƒç”¨...")
            start_time = time.time()
            
            # æ ¹æ®æ¨¡å‹é€‰æ‹©æ­£ç¡®çš„tokenå‚æ•°
            token_param = self._get_token_param_name()
            api_params = {
                "model": self.model_name,
                token_param: 500,
                "messages": [
                    {
                        "role": "user",
                        "content": "ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
                    }
                ]
            }
            
            # åªæœ‰åœ¨æ¨¡å‹æ”¯æŒçš„æƒ…å†µä¸‹æ‰è®¾ç½®temperature
            if not self._should_use_default_temperature():
                api_params["temperature"] = 0.3
            
            response = self.client.chat.completions.create(**api_params)
            
            end_time = time.time()
            
            result = {
                "success": True,
                "provider": self.provider,
                "response_time": end_time - start_time,
                "total_tokens": response.usage.total_tokens,
                "input_tokens": response.usage.prompt_tokens,
                "output_tokens": response.usage.completion_tokens,
                "response_length": len(response.choices[0].message.content),
                "response_content": response.choices[0].message.content,
                "model_name": self.model_name
            }
            
            print(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ!")
            print(f"â±ï¸  å“åº”æ—¶é—´: {result['response_time']:.2f}ç§’")
            print(f"ğŸ”¢ ä½¿ç”¨ä»¤ç‰Œ: {result['total_tokens']}")
            print(f"ğŸ“ å“åº”é•¿åº¦: {result['response_length']}å­—ç¬¦")
            print(f"ğŸ“„ å“åº”é¢„è§ˆ:")
            preview = result['response_content'][:200] + "..." if len(result['response_content']) > 200 else result['response_content']
            print(preview)
            
            return result
            
        except Exception as e:
            print(f"âŒ OpenAIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            return {
                "success": False,
                "provider": self.provider,
                "error": str(e),
                "model_name": self.model_name
            }
    
    def create_message(self, user_message: str, system_prompt: str = None, 
                      max_tokens: int = 20000, temperature: float = 0.7) -> Dict[str, Any]:
        """åˆ›å»ºOpenAIæ¶ˆæ¯"""
        return self._create_message_with_retry(user_message, system_prompt, max_tokens, temperature)
    
    def _create_message_with_retry(self, user_message: str, system_prompt: str = None, 
                                  max_tokens: int = 20000, temperature: float = 0.7, 
                                  max_retries: int = 3) -> Dict[str, Any]:
        """å¸¦é‡è¯•æœºåˆ¶çš„æ¶ˆæ¯åˆ›å»º"""
        import time
        import random
        
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                
                # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
                messages = []
                if system_prompt:
                    messages.append({
                        "role": "system",
                        "content": system_prompt
                    })
                
                messages.append({
                    "role": "user",
                    "content": user_message
                })
                
                # æ ¹æ®æ¨¡å‹é€‰æ‹©æ­£ç¡®çš„tokenå‚æ•°
                token_param = self._get_token_param_name()
                api_params = {
                    "model": self.model_name,
                    token_param: max_tokens,
                    "messages": messages
                }
                
                # åªæœ‰åœ¨æ¨¡å‹æ”¯æŒçš„æƒ…å†µä¸‹æ‰è®¾ç½®temperature
                if not self._should_use_default_temperature():
                    api_params["temperature"] = temperature
                
                # è°ƒç”¨API
                response = self.client.chat.completions.create(**api_params)
                
                end_time = time.time()
                
                return {
                    "success": True,
                    "provider": self.provider,
                    "response_time": end_time - start_time,
                    "input_tokens": response.usage.prompt_tokens,
                    "output_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens,
                    "response_content": response.choices[0].message.content,
                    "response_length": len(response.choices[0].message.content),
                    "model_name": self.model_name,
                    "system_prompt": system_prompt,
                    "user_message": user_message,
                    "system_prompt_length": len(system_prompt) if system_prompt else 0,
                    "user_message_length": len(user_message),
                    "retry_attempt": attempt + 1
                }
                
            except Exception as e:
                error_str = str(e)
                print(f"âš ï¸  APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_str}")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯429é”™è¯¯
                if "429" in error_str:
                    if attempt < max_retries - 1:
                        # å¦‚æœæ˜¯429é”™è¯¯ä¸”è¿˜æœ‰é‡è¯•æœºä¼šï¼Œå°è¯•é™çº§ç­–ç•¥
                        if attempt == 0:
                            # ç¬¬ä¸€æ¬¡é‡è¯•ï¼šå‡å°‘max_tokens
                            max_tokens = min(max_tokens // 2, 8000)
                            print(f"ğŸ”„ é™çº§ç­–ç•¥1: å‡å°‘max_tokensåˆ° {max_tokens}")
                        elif attempt == 1:
                            # ç¬¬äºŒæ¬¡é‡è¯•ï¼šä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
                            original_model = self.model_name
                            if "gpt-4" in self.model_name:
                                self.model_name = "gpt-3.5-turbo"
                                print(f"ğŸ”„ é™çº§ç­–ç•¥2: åˆ‡æ¢åˆ° {self.model_name}")
                        
                        # ç­‰å¾…æ—¶é—´ï¼šæŒ‡æ•°é€€é¿ + éšæœºå»¶è¿Ÿ
                        wait_time = (2 ** attempt) + random.uniform(1, 3)
                        print(f"â° ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                
                # å…¶ä»–é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°ç”¨å®Œ
                return {
                    "success": False,
                    "provider": self.provider,
                    "error": error_str,
                    "model_name": self.model_name,
                    "system_prompt": system_prompt,
                    "user_message": user_message,
                    "retry_attempts": attempt + 1
                }
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        return {
            "success": False,
            "provider": self.provider,
            "error": f"æ‰€æœ‰ {max_retries} æ¬¡é‡è¯•éƒ½å¤±è´¥",
            "model_name": self.model_name,
            "system_prompt": system_prompt,
            "user_message": user_message,
            "retry_attempts": max_retries
        } 